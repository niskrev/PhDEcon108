{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8bbadd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time series concepts and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc9296",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Definition__: A time series is a collection of random variables indexed by time , i.e. \n",
    "\n",
    "$$ \\{z_t: t =~ ..., -2, -1, ~0,~1, ~2,~ ... \\} = \\{z_t \\}_{t=-\\infty}^{\\infty} $$ (equation-time-series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb28bf8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this class we will only deal with discrete time series, i.e. where $t$ is discrete as above. This is common in macroeconomics. In finance, continuous time series are often more useful/relevant. Equation {eq}`equation-time-series` is also refered to stochastic process, or stochastic sequence (some authors reserve sequence for discrete and process -- for continuous time series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970791bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* unlike cross-sectional data, time series are ordered <u>sequentially</u> - there is _before_ and _after_ and observations can be close or far away from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eb49c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In practice, we work with observed finite realizations of stochastic processes:\n",
    "\n",
    "    $$\\{ z_1, z_2, ..., z_T \\} = \\{z_t\\}_{t=1}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e755820",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{note}\n",
    "We use __time series__ for both the process and the realization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2656928",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the set $\\{ z_1, z_2, ..., z_T \\}$ is characterized by its joint distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34495736",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the distribution of the full stochastic process is specified if for an arbitrary set of indices we know the joint distribution of the respective set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5c23d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Example__: $\\{z_t \\}_{t=-\\infty}^{\\infty}$ is a Gaussian process if any finite subset $\\{ z_{t_1}, z_{t_2}, ..., z_{t_k} \\}$ has a joint Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a735eab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A time series sample $\\{z_t\\}_{t=1}^T$ is a <u> single </u> draw from some distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701813d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sample size of 1???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70fbeeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* how do we estimate moments (learn about the underlying distribution)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb07e1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* in cross-section: data points are independent draws from a common distribution\n",
    "\n",
    "    + in $z_i = \\begin{bmatrix}y_i\\\\x_i\\end{bmatrix} $ there could be arbitrarily complicated dependence, but $z_i$ and $z_j$ are independent for $i \\neq j$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696a0f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* sample averages estimate population means    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368e295",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the equivalent in time series is observing muliple ensembles (paths) of the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf08f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* and computing __ensemble averages__ for each $t$\n",
    "\n",
    "$$ \\frac{1}{N} \\sum_{n=1}^{N} z^{(n)}_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25f91b",
   "metadata": {},
   "source": [
    "![Erg1](./images/Erg1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7665f15",
   "metadata": {},
   "source": [
    "![Erg2](./images/Erg2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac435b65",
   "metadata": {},
   "source": [
    "Ensamble average with n=100\n",
    "\n",
    "![Erg1](./images/Erg_mean_n_100.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3730c",
   "metadata": {},
   "source": [
    "Ensamble average with n=10000\n",
    "\n",
    "![Erg1](./images/Erg_mean_n_10000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed8724",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* impossible with one observed path (e.g. one history of quarterly GDP numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a54aa5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* can only compute averages over time\n",
    "\n",
    "$$ \\displaystyle{\\frac{1}{T} \\sum_{t=1}^{T} z_{t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d656cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* for __time averages__ to estimate population means, we need ergodic stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ed563",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ergodic stationarity\n",
    "\n",
    "* With cross-sectional data we have identical distribution and independence\n",
    "* the related concepts for time series are stationarity and ergodicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42842114",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5779d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition__: The process $\\{z_t\\}_{t=1}^T$ is ___strictly (or strongly) stationary___ if its distribution is time invariant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a6e17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that if we take an arbitraty set of time indices ${t_1, t_2, .., t_k}$ for some $k$, the joint distribution of $z_{t_1}, z_{t_2}, ,..., z_{t_k}$ stays the same if we shift all indices by the same number of time units, i.e. it is the same as the joint distribution of  $z_{t'_1}, z_{t'_2}, ,..., z_{t'_k}$ for $t' = t + h$ for any $h$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ba647",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition__:  The process $\\{z_t\\}_{t=1}^T$ is ___covariance (or weakly) stationary___ if the first two moments of the joint distribution exist and are time invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e607ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, the mean and covariance do not change with the time index: \n",
    "\n",
    "$$\\operatorname{E}z_t=\\operatorname{const} < \\infty$$\n",
    "\n",
    "$$ \\operatorname{cov}(z_t, z_{t+k}) =\\operatorname{cov}(z_{t'}, z_{t'+k}) < \\infty$$ \n",
    "\n",
    "for $t' = t+h$, $k \\geq 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2f857",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* also wide-sense stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f4d69",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* with $k=0$ the variance of $z_t$ is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ac543",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "set $t'=t-k$. Then\n",
    "\n",
    "$$ \\operatorname{cov}(z_t, z_{t+k}) =\\operatorname{cov}(z_{t-k}, z_{t})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e19f52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* stationarity is a form of constancy of the properties of the process\n",
    "* needed in order to be able to learn something about those properties\n",
    "* strong stationarity is usually too strong an assumption\n",
    "* covariance stationarity is typically enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe158c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* if the first two moments exist, strong stationarity  implies weak stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475ead8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "__Example__: (strictly but not weakly stationary)\n",
    "\n",
    "iid $z_t$ with a Cauchy distribution is strictly stationary but has no finite moments and is therefore not weakly stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2970d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* strong and weak stationarity coincide when $z_t$ is a Gaussian process\n",
    "* with weak stationarity alone we can still learn first and second order moments (if not the full distribution)\n",
    "* if $z_t$ is iid then it is strongly stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f0948",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ergodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520db98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* independence of observations implies each contains <u>unique</u> information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed3db8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* dependence means that some information is <u>shared</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa17ae0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* for information to <u>accumulate</u> as sample size grows, some unique information must exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c229a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* i.e. dependence must not be too strong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6ecaa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* __ergodicity__ is the property that more distant variables in the sequence are closer to being independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe1ded",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* (__Ergodic theorem__) For ergodic and stationary processes, time averages converge to population means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92279b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* functions of ergodic and stationary processes are also ergodic and stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56ecad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* time averages of such functions converge to respective population averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825265f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Example__: If $\\{z_t\\}$ is stationary and ergodic, then $\\{z_t z_t\\}$ is also stationary and ergodic.<br>\n",
    "Therefore, the time averages of $z_t$ and $z_t z_t$ converge to $\\operatorname{E}z_t$ and $\\operatorname{E}z_tz_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e99924",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Example__: (stationary but not ergodic)\n",
    "\n",
    "$$ z_t = x_t + y$$\n",
    "    \n",
    "with:\n",
    "* $x_t$ iid, $\\operatorname{E}x_t = 0$\n",
    "* $y$ independent of $x_t$ and $\\operatorname{E}y = 0$, $\\operatorname{var}(y) > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6578ecb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* since $x_t$ is iid:\n",
    "\n",
    "$$ \\displaystyle{\\frac{1}{T} \\sum_{t=1}^{T} x_{t}} \\longrightarrow \\operatorname{E}x_t = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b27a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* since $y$ is the same for all $t$: \n",
    "\n",
    "$$ \\displaystyle{\\frac{1}{T} \\sum_{t=1}^{T} y} \\longrightarrow y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e5862",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore:\n",
    "\n",
    "$$ \\displaystyle{\\frac{1}{T} \\sum_{t=1}^{T} z_t} \\longrightarrow y$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae66109",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the other hand, the (ensemble) mean of $z_t$ is\n",
    "\n",
    "$$\\operatorname{E}z_t = \\operatorname{E}x_t + \\operatorname{E}y = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baba706",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is because dependence between $z_t$ and $z_{t-k}$ does not decrease as $k$ increases:\n",
    "\n",
    "$$ \\operatorname{cov}(z_t, z_{t-k}) = \\operatorname{var}(y) $$\n",
    "\n",
    "due to the independence among $x_t$, $x_{t-k}$, and $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bed071",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```{important}\n",
    "The ergodic theorem is important because it implies that, in time series settings, a single long sample becomes representative of the whole data-generating process similar to how, in a cross section, a large iid sample becomes representative of the whole population.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e98c1",
   "metadata": {},
   "source": [
    "```{note}\n",
    "A related (to ergodicity) property is called _mixing_. Defining formally either one is beyond the scope of this course. Informally, a useful way to think about the difference between mixing and ergodicity can be found in Ch 14.7 of B. Hansens's _Econometrics_: mixing means that more distant elements of the time series sequence are closer to being independent, while ergodicity means that this is true only on average. Therefore, mixing implies ergodicity, but ergodicity does not imply mixing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e458963",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269558f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* sample\n",
    "\n",
    "$$\\{ z_1, z_2, ..., \\}$$\n",
    "\n",
    "* in cross section - independent\n",
    "* in time series - dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0560ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What does it mean for two variables $z_1$ and $z_2$ to be independent? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebee489",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Let $f(z_1, z_2)$ be the joint distribution of $z_1$ and $z_2$. If $z_1$ and $z_2$ are independent, then\n",
    "\n",
    "$$f(z_1, z_2) = f(z_1)f(z_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c247d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Moreover, from the definition of conditional distribution\n",
    "\n",
    "$$f(z_1 | z_2) = \\frac{f(z_1, z_2)}{f(z_2)}  ~~\\text{and}~~ f(z_2 | z_1) = \\frac{f(z_1, z_2)}{f(z_1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9a0ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it follows that if $z_1$ and $z_2$ are independent, \n",
    "\n",
    "$$f(z_1 | z_2) = f(z_1) ~~\\text{and}~~ f(z_2 | z_1) = f(z_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f7bf0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When $z_1$ and $z_2$ are independent\n",
    "\n",
    "* observing $z_2$ tells us nothing about $z_1$, no new information is gained, beyond the one already contained in the marginal distribution of $z_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e300fe8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This is a defining property of a random sample: each observation is independent from all other observations, and is therefore an unique source of information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81c439",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When $z_1$ and $z_2$ are dependent\n",
    "\n",
    "$$f(z_1 | z_2) \\neq f(z_1)$$\n",
    "\n",
    "* observing $z_2$ tells us something about $z_1$, and vice versa -- $z_1$ is informative about $z_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b4410d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* if all $z$'s are mutually dependent, the larger the number of observations, the smaller is the information value of each individual observation, given all other observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64fbd2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Time series models are largely models of the temporal dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15000d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This has various important implications. One is that standard results from probability theory, such as the law of large numbers or the central limit theorem, are not directly applicable. \n",
    "Another is that we need to somehow model the temporal dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd39b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time series models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c0176",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* aim to capture complex temporal dependence\n",
    "* build by combining processes with simple dependence structure - __innovations__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732047f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Innovations\n",
    "\n",
    "$$ \\{ \\varepsilon_t \\}, \\;\\;\\; \\operatorname{E} (\\varepsilon_t) = 0, \\;\\;\\; \\operatorname{var}(\\varepsilon_t) = \\sigma^2$$\n",
    "\n",
    "* Gaussian iid noise\n",
    "* iid noise\n",
    "* stationary martingale difference\n",
    "* white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823dc93",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition:__ The innovation process $\\{\\varepsilon_t\\}$ is an ___(Gaussian) iid noise___ process if it is iid (and Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6b430",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "time series model for the observed data ${x_t}$ is a specification of the joint\n",
    "distributions (or possibly only the means and covariances) of a sequence of random\n",
    "variables $\\{Z_t\\}$ of which $\\{z_t\\}$ is postulated to be a realization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274577bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition__: The innovation process $\\{\\varepsilon_t\\}$ is a ___martingale difference___ process if \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{E}\\left( \\varepsilon_t | \\varepsilon_{t-1},\\varepsilon_{t-2},... \\right) &= 0\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071571c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\operatorname{E}\\left( \\varepsilon_t \\right) = 0$ (by LIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a8fd5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\operatorname{cov}(\\varepsilon_t, \\varepsilon_{t-h}) =0 $, for all $h\\neq0$ (also by LIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce5d158",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* if also covariance stationary: $ \\operatorname{var}(\\varepsilon_t) = \\sigma^2_{t} = \\sigma^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4dc45a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Definition__: The innovation process $\\{\\varepsilon_t\\}$ is a ___white noise___ process if \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{var}(\\varepsilon_t)&=\\sigma^2 < \\infty\\\\\n",
    "\\operatorname{cov}(\\varepsilon_t, \\varepsilon_{t-h}) &= 0, ~~h \\neq 0\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf70d03",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* stronger than covariance stationary (uncorrelated)\n",
    "* weaker than iid noise (not necessarily independent or identically distributed)\n",
    "* weaker than stationary martingale difference\n",
    "* uncorrelated and Gaussian => Gaussian iid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1929a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forms of (in)dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476119b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* with iid innovations the future is completely independent from the past history\n",
    "    + the past contains no information about the future (completely unpredictable)\n",
    "    + conditional distribution equal to the unconditional one\n",
    "* with m.d. innovations the mean in the future is completely independent from the past history\n",
    "    + the past contains no information about the mean (the mean is unpredictable)\n",
    "    + conditional mean equals the unconditional mean \n",
    "* for white noise innovations the mean in the future is linearly independent from the past history\n",
    "    + linear functions of the past contain no information about the mean (the mean is linearly unpredictable)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74522281",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of time series models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c7061",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__MA(1) model__\n",
    "\n",
    "$$ z_t = \\varepsilon_t + \\theta \\varepsilon_{t-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65f9e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__AR (1) model__: \n",
    "\n",
    "  $$z_t = \\alpha z_{t-1} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395636b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__ARMA(1,1) model:__\n",
    "\n",
    "   $$z_t = \\alpha z_{t-1} + \\varepsilon_t + \\theta \\varepsilon_{t-1}$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
